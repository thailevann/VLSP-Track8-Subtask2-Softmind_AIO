{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-09-04T09:55:48.172229Z",
     "iopub.status.busy": "2025-09-04T09:55:48.171953Z",
     "iopub.status.idle": "2025-09-04T09:56:01.706958Z",
     "shell.execute_reply": "2025-09-04T09:56:01.705950Z",
     "shell.execute_reply.started": "2025-09-04T09:55:48.172207Z"
    },
    "id": "qghLv4T1fswr",
    "outputId": "d6310998-60f7-4e44-a35b-76784b4ec853",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth\n",
    "else:\n",
    "    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n",
    "    !pip install --no-deps unsloth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T09:56:26.942463Z",
     "iopub.status.busy": "2025-09-04T09:56:26.941885Z",
     "iopub.status.idle": "2025-09-04T09:56:27.032249Z",
     "shell.execute_reply": "2025-09-04T09:56:27.031658Z",
     "shell.execute_reply.started": "2025-09-04T09:56:26.942438Z"
    },
    "id": "HJn9sExOfswy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"YOUR-KEY-HUGGING-FACE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8rT5exXSfswz"
   },
   "source": [
    "#**Finetune**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T09:56:29.419474Z",
     "iopub.status.busy": "2025-09-04T09:56:29.419240Z",
     "iopub.status.idle": "2025-09-04T09:57:32.988896Z",
     "shell.execute_reply": "2025-09-04T09:57:32.987972Z",
     "shell.execute_reply.started": "2025-09-04T09:56:29.419455Z"
    },
    "id": "ciOEdk6dfsw0",
    "outputId": "7973ba7e-ecc8-4676-954c-4251953d5911",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-04 09:56:41.717433: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1756979802.109986      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1756979802.221755      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.8.10: Fast Qwen3 patching. Transformers: 4.52.4.\n",
      "   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 6.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38293ef224d647dd940e9c757f82629a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/3.51G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "530a45492d39472f8b074f0462622fe4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826cdd76381b4b7c825bd9f5435244cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68947bc605834820942e53e0406c8bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01eef6569a1d4878be57b6fa399bca3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99f8539da6df40b48cce32eeb29532c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead7e0d85e25463c91dca9cf49856669",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6bc551ce00147d08cbcbcc88de32f2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "103498242a844cf4abe733f5be1f457a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chat_template.jinja: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Qwen3-4B-Thinking-2507-unsloth-bnb-4bit\",\n",
    "    max_seq_length = 1024,   # Context length - can be longer, but uses more memory\n",
    "    load_in_4bit = True,     # 4bit uses much less memory\n",
    "    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
    "    full_finetuning = False, # We have full finetuning now!\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T09:57:58.414984Z",
     "iopub.status.busy": "2025-09-04T09:57:58.414700Z",
     "iopub.status.idle": "2025-09-04T09:57:58.422372Z",
     "shell.execute_reply": "2025-09-04T09:57:58.421631Z",
     "shell.execute_reply.started": "2025-09-04T09:57:58.414964Z"
    },
    "id": "rolPVU83fsw1",
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: Already have LoRA adapters! We shall skip this step.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "        r = 32,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
    "        target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                          \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "        lora_alpha = 32,  # Best to choose alpha = rank or rank*2\n",
    "        lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "        bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "        # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "        use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "        random_state = 3407,\n",
    "        use_rslora = False,   # We support rank stabilized LoRA\n",
    "    loftq_config = None,  # And LoftQ\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T09:57:59.996646Z",
     "iopub.status.busy": "2025-09-04T09:57:59.995878Z",
     "iopub.status.idle": "2025-09-04T09:58:00.001775Z",
     "shell.execute_reply": "2025-09-04T09:58:00.000942Z",
     "shell.execute_reply.started": "2025-09-04T09:57:59.996622Z"
    },
    "id": "h_aFvTD5fsw1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def remove_markdown(text: str) -> str:\n",
    "    # Lo·∫°i b·ªè t√¥ ƒë·∫≠m **text**\n",
    "    text = re.sub(r'\\*\\*(.*?)\\*\\*', r'\\1', text)\n",
    "\n",
    "    # Lo·∫°i b·ªè bullet point b·∫Øt ƒë·∫ßu b·∫±ng *, -, ho·∫∑c + v√† kho·∫£ng tr·∫Øng sau ƒë√≥\n",
    "    text = re.sub(r'^\\s*[\\*\\-\\+]\\s+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Lo·∫°i b·ªè ti√™u ƒë·ªÅ markdown nh∆∞ ###, ####\n",
    "    text = re.sub(r'^#{1,6}\\s*', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Thay th·∫ø nhi·ªÅu d√≤ng li√™n t·ª•c b·∫±ng d√≤ng tr·ªëng\n",
    "    text = re.sub(r'\\n+', '\\n', text)\n",
    "\n",
    "    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng d∆∞ th·ª´a ·ªü ƒë·∫ßu v√† cu·ªëi\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T09:58:01.904135Z",
     "iopub.status.busy": "2025-09-04T09:58:01.903853Z",
     "iopub.status.idle": "2025-09-04T09:58:01.909168Z",
     "shell.execute_reply": "2025-09-04T09:58:01.908428Z",
     "shell.execute_reply.started": "2025-09-04T09:58:01.904114Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"qwen3-thinking\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-04T09:58:03.988342Z",
     "iopub.status.busy": "2025-09-04T09:58:03.988067Z",
     "iopub.status.idle": "2025-09-04T09:58:06.900322Z",
     "shell.execute_reply": "2025-09-04T09:58:06.899542Z",
     "shell.execute_reply.started": "2025-09-04T09:58:03.988323Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "819469a39503495c9649bc17fe6c1d11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/607 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7ad3a05fcb4255ba183fb13081c1b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001.parquet:   0%|          | 0.00/1.31M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fc655edba6e45deb093598439affa5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/test-00000-of-00001.parquet:   0%|          | 0.00/246k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad99456efde34b96a75936ccd9ba1225",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/1250 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d9535b905347b7b45ba830f6ada27f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/221 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"thailevann/durationQA_track8_vlsp2025\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T10:40:48.042763Z",
     "iopub.status.busy": "2025-09-03T10:40:48.042183Z",
     "iopub.status.idle": "2025-09-03T10:40:48.241800Z",
     "shell.execute_reply": "2025-09-03T10:40:48.241248Z",
     "shell.execute_reply.started": "2025-09-03T10:40:48.042742Z"
    },
    "id": "AAcoukedfsw2",
    "outputId": "420b3ca6-07fa-41ee-9ed1-07d62bcbbd56",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "'''\n",
    "# 1. Load d·ªØ li·ªáu t·ª´ JSONL th√†nh list of dicts\n",
    "path = '/kaggle/working/duration_reasoning_output.jsonl'\n",
    "data = [json.loads(line) for line in open(path, 'r', encoding='utf-8')]\n",
    "\n",
    "# 2. Chuy·ªÉn th√†nh Huggingface Dataset\n",
    "dataset = Dataset.from_list(data)\n",
    "# 4. √Åp d·ª•ng h√†m cho to√†n b·ªô dataset\n",
    "dataset = dataset.map(add_sorted_options_reason)\n",
    "'''\n",
    "\n",
    "# 5. Ki·ªÉm tra k·∫øt qu·∫£\n",
    "#print(dataset['train'][0]['CoT'])\n",
    "def generate_conversation(batch):\n",
    "    conversations = []\n",
    "    for i in range(len(batch[\"question\"])):\n",
    "        question  = batch[\"question\"][i]\n",
    "        reasoning = remove_markdown(batch[\"CoT\"][i])\n",
    "        options   = batch[\"options\"][i]\n",
    "        labels    = batch[\"labels\"][i]\n",
    "        context   = batch[\"context\"][i]\n",
    "\n",
    "        # Format l·ª±a ch·ªçn\n",
    "        options_text = \"\\n\".join([f\"{idx+1}. {opt}\" for idx, opt in enumerate(options)])\n",
    "\n",
    "        problem = (\n",
    "                    f\"{context}\\n\\n\"\n",
    "                    f\"C√¢u h·ªèi: {question}\\n\"\n",
    "                    f\"L·ª±a ch·ªçn:\\n{options_text}\\n\\n\"\n",
    "                    f\"H√£y suy lu·∫≠n theo **Chain-of-Thought (CoT)** ƒë·ªÉ t√¨m ra l·ª±a ch·ªçn ƒë√∫ng.\\n\"\n",
    "                    f\"Vi·∫øt ph·∫ßn suy lu·∫≠n v√†o th·∫ª <think>.  Sau ƒë√≥, li·ªát k√™ nh√£n t∆∞∆°ng ·ª©ng c·ªßa t·ª´ng l·ª±a ch·ªçn trong th·∫ª <labels>.\\n\"\n",
    "        )\n",
    "        # L·ªùi gi·∫£i\n",
    "        solution = (\n",
    "            f\"<think>\\n{reasoning.strip()}\\n</think>\\n\"\n",
    "            f\"<labels>{', '.join([label.strip().lower() for label in labels])}</labels>\"\n",
    "        )\n",
    "\n",
    "        conversations.append([\n",
    "            {\"role\": \"user\", \"content\": problem},\n",
    "            {\"role\": \"assistant\", \"content\": solution},\n",
    "        ])\n",
    "\n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "\n",
    "reasoning_conversations = tokenizer.apply_chat_template(\n",
    "    dataset['train'].map(generate_conversation, batched = True)[\"conversations\"],\n",
    "    tokenize = False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T10:44:11.483706Z",
     "iopub.status.busy": "2025-09-03T10:44:11.483441Z",
     "iopub.status.idle": "2025-09-03T10:44:11.786108Z",
     "shell.execute_reply": "2025-09-03T10:44:11.785565Z",
     "shell.execute_reply.started": "2025-09-03T10:44:11.483690Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "import json\n",
    "'''\n",
    "# 1. Load d·ªØ li·ªáu t·ª´ JSONL th√†nh list of dicts\n",
    "path = '/kaggle/working/duration_reasoning_output.jsonl'\n",
    "data = [json.loads(line) for line in open(path, 'r', encoding='utf-8')]\n",
    "\n",
    "# 2. Chuy·ªÉn th√†nh Huggingface Dataset\n",
    "dataset = Dataset.from_list(data)\n",
    "dataset = dataset.map(add_sorted_options_reason)\n",
    "'''\n",
    "\n",
    "def nongenerate_conversation(batch):\n",
    "    conversations = []\n",
    "    for i in range(len(batch[\"question\"])):\n",
    "        question  = batch[\"question\"][i]\n",
    "        reasoning = remove_markdown(batch[\"CoT\"][i])\n",
    "        options   = batch[\"options\"][i]\n",
    "        labels    = batch[\"labels\"][i]\n",
    "        context   = batch[\"context\"][i]\n",
    "\n",
    "        # Format l·ª±a ch·ªçn\n",
    "        options_text = \"\\n\".join([f\"{idx+1}. {opt}\" for idx, opt in enumerate(options)])\n",
    "        problem = (\n",
    "            f\"{context}\\n\\n\"\n",
    "            f\"C√¢u h·ªèi: {question}\\n\"\n",
    "            f\"Suy lu·∫≠n chi ti·∫øt cho t·ª´ng l·ª±a ch·ªçn:\\n{reasoning.strip()}\\n\"\n",
    "            f\"C√°c l·ª±a ch·ªçn:\\n{options_text}\\n\\n\"\n",
    "            \"Nhi·ªám v·ª•:\\n\"\n",
    "            \"1. ƒê·ªçc ph·∫ßn suy lu·∫≠n c·ªßa t·ª´ng l·ª±a ch·ªçn.\\n\"\n",
    "            \"2. Ch·ªâ g√°n nh√£n 1 cho ph∆∞∆°ng √°n ƒë∆∞·ª£c t·ªïng k·∫øt l√† h·ª£p l√Ω nh·∫•t.\\n\"\n",
    "            \"3. T·∫•t c·∫£ c√°c l·ª±a ch·ªçn kh√°c g√°n nh√£n 0, ngay c·∫£ khi suy lu·∫≠n n√≥i 'c√≥ th·ªÉ h·ª£p l√Ω trong ƒëi·ªÅu ki·ªán ƒë·∫∑c bi·ªát'.\\n\"\n",
    "            \"4. K·∫øt qu·∫£ ph·∫£i ho√†n to√†n nh·∫•t qu√°n v·ªõi ph·∫ßn t·ªïng k·∫øt suy lu·∫≠n.\\n\"\n",
    "            \"5. N·∫øu d·ª±a v√†o context v√† question m√† kh√¥ng th·ªÉ x√°c ƒë·ªãnh option n√†o h·ª£p l√Ω nh·∫•t, g√°n 0 cho t·∫•t c·∫£ c√°c option.\\n\"\n",
    "            \"6. Li·ªát k√™ nh√£n t∆∞∆°ng ·ª©ng c·ªßa t·ª´ng l·ª±a ch·ªçn theo ƒë√∫ng th·ª© t·ª±, trong th·∫ª <labels>, v√≠ d·ª•: <labels>1 0 0 0</labels>.\"\n",
    "        )\n",
    "\n",
    "        # L·ªùi gi·∫£i\n",
    "        solution = (\n",
    "            f\"<labels>{', '.join([label.strip().lower() for label in labels])}</labels>\"\n",
    "        )\n",
    "\n",
    "        conversations.append([\n",
    "            {\"role\": \"user\", \"content\": problem},\n",
    "            {\"role\": \"assistant\", \"content\": solution},\n",
    "        ])\n",
    "\n",
    "    return {\"conversations\": conversations}\n",
    "\n",
    "\n",
    "nonreasoning_conversations = tokenizer.apply_chat_template(\n",
    "    dataset['train'].map(nongenerate_conversation, batched = True)[\"conversations\"],\n",
    "    tokenize = False,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T10:46:02.075607Z",
     "iopub.status.busy": "2025-09-03T10:46:02.075056Z",
     "iopub.status.idle": "2025-09-03T10:46:02.162150Z",
     "shell.execute_reply": "2025-09-03T10:46:02.161424Z",
     "shell.execute_reply.started": "2025-09-03T10:46:02.075583Z"
    },
    "id": "pB8hEDErfsw6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "data = pd.concat([\n",
    "    pd.Series(reasoning_conversations),\n",
    "    pd.Series(nonreasoning_conversations)\n",
    "\n",
    "])\n",
    "data.name = \"text\"\n",
    "\n",
    "from datasets import Dataset\n",
    "combined_dataset = Dataset.from_pandas(pd.DataFrame(data))\n",
    "combined_dataset = combined_dataset.shuffle(seed = 3407)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T10:46:27.635030Z",
     "iopub.status.busy": "2025-09-03T10:46:27.634275Z",
     "iopub.status.idle": "2025-09-03T10:46:34.993740Z",
     "shell.execute_reply": "2025-09-03T10:46:34.992844Z",
     "shell.execute_reply.started": "2025-09-03T10:46:27.634980Z"
    },
    "id": "DnHv0kiCfsw7",
    "outputId": "c7684071-e020-49b4-9aa0-9f6ba66ee32f",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=combined_dataset,\n",
    "    args=SFTConfig(\n",
    "        dataset_text_field=\"text\",\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        num_train_epochs=2,\n",
    "        learning_rate=2e-5,\n",
    "        warmup_ratio=0.03,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"epoch\",\n",
    "        #evaluation_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        #load_best_model_at_end=True,\n",
    "        fp16=True,  # ho·∫∑c bf16 n·∫øu h·ªó tr·ª£\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T10:46:39.188981Z",
     "iopub.status.busy": "2025-09-03T10:46:39.188674Z",
     "iopub.status.idle": "2025-09-03T10:46:41.393167Z",
     "shell.execute_reply": "2025-09-03T10:46:41.392326Z",
     "shell.execute_reply.started": "2025-09-03T10:46:39.188951Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from unsloth.chat_templates import train_on_responses_only\n",
    "trainer = train_on_responses_only(\n",
    "    trainer,\n",
    "    instruction_part = \"<|im_start|>user\\n\",\n",
    "    response_part = \"<|im_start|>assistant\\n\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-03T10:47:15.652479Z",
     "iopub.status.busy": "2025-09-03T10:47:15.651823Z",
     "iopub.status.idle": "2025-09-03T10:47:15.658210Z",
     "shell.execute_reply": "2025-09-03T10:47:15.657536Z",
     "shell.execute_reply.started": "2025-09-03T10:47:15.652450Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# @title Show current memory stats\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
    "print(f\"{start_gpu_memory} GB of memory reserved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#tokenizer.decode(trainer.train_dataset[100][\"input_ids\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#tokenizer.decode([tokenizer.pad_token_id if x == -100 else x for x in trainer.train_dataset[100][\"labels\"]]).replace(tokenizer.pad_token, \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOARsSFDfsw7",
    "outputId": "d5efb319-15ce-4e8d-caa5-eb246e0a5b59",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "trainer_stats = trainer.train()\n",
    "model.push_to_hub(\"thailevann/cot_refine\") \n",
    "tokenizer.push_to_hub(\"thailevann/cot_refine\") \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
