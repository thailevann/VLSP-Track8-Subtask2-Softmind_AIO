{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\nimport os\nif \"COLAB_\" not in \"\".join(os.environ.keys()):\n    !pip install unsloth\nelse:\n    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n    !pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer\n    !pip install --no-deps unsloth\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-09T07:22:21.965265Z","iopub.execute_input":"2025-09-09T07:22:21.965761Z","iopub.status.idle":"2025-09-09T07:22:35.657311Z","shell.execute_reply.started":"2025-09-09T07:22:21.965732Z","shell.execute_reply":"2025-09-09T07:22:35.656451Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(token=\"YOUR-TOKEN-HUGINGFACE\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T07:22:59.554974Z","iopub.execute_input":"2025-09-09T07:22:59.555591Z","iopub.status.idle":"2025-09-09T07:22:59.992667Z","shell.execute_reply.started":"2025-09-09T07:22:59.555555Z","shell.execute_reply":"2025-09-09T07:22:59.991916Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\n\nmodel1, tokenizer1 = FastLanguageModel.from_pretrained(\n    model_name = \"thailevann/cot_refine\",\n    max_seq_length = 1024,\n    load_in_4bit = True,\n    load_in_8bit = False,\n    full_finetuning = False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T07:23:01.096479Z","iopub.execute_input":"2025-09-09T07:23:01.096739Z","iopub.status.idle":"2025-09-09T07:24:11.788400Z","shell.execute_reply.started":"2025-09-09T07:23:01.096719Z","shell.execute_reply":"2025-09-09T07:24:11.787557Z"}},"outputs":[{"name":"stdout","text":"ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-09-09 07:23:14.516294: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757402594.816829      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757402594.886679      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"ü¶• Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.9.2: Fast Qwen3 patching. Transformers: 4.52.4.\n   \\\\   /|    Tesla P100-PCIE-16GB. Num GPUs = 1. Max memory: 15.888 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 6.0. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/3.51G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3acb8db2e954b45aa9018318c37ce94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/238 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a74be00b43014cd381fb30865691bb56"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54d3e7ae579041569c3b1f535a136b88"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"681ae9e62c034da2ab68d26ef607584c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1691efb4b13f41978e9f1fb66e62f031"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/707 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bcd12021a984402a61146debe23c60d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/614 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eda4bee0131644d8bb92787cf401507a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d621d17110ec4c16be3d7eb52c785195"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4865bbc599a40f8a82d13d73776ca83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/264M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d8337664093409db18cdbba319ede39"}},"metadata":{}},{"name":"stderr","text":"Unsloth 2025.9.2 patched 36 layers with 36 QKV layers, 36 O layers and 36 MLP layers.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\ndef generate_answer_cot(context, question, options):\n    options_text = \"\\n\".join([f\"{i+1}. {opt}\" for i, opt in enumerate(options)])\n\n\n    problem = (\n                    f\"{context}\\n\\n\"\n                    f\"C√¢u h·ªèi: {question}\\n\"\n                    f\"L·ª±a ch·ªçn:\\n{options_text}\\n\\n\"\n                    f\"H√£y suy lu·∫≠n theo **Chain-of-Thought (CoT)** ƒë·ªÉ t√¨m ra l·ª±a ch·ªçn ƒë√∫ng.\\n\"\n                    f\"Vi·∫øt ph·∫ßn suy lu·∫≠n v√†o th·∫ª <think>.  Sau ƒë√≥, li·ªát k√™ nh√£n t∆∞∆°ng ·ª©ng c·ªßa t·ª´ng l·ª±a ch·ªçn trong th·∫ª <labels>.\\n\"\n        )\n\n\n\n    messages = [{\"role\": \"user\", \"content\": problem}]\n    text = tokenizer1.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True,\n            enable_thinking=True,\n    )\n    # Tokenize & gen\n    inputs = tokenizer1(text, return_tensors=\"pt\").to(\"cuda\")\n    '''\n    output_ids = model1.generate(\n        **inputs,\n        max_new_tokens=1024,\n        temperature=0.6,\n        top_p=0.95,\n        top_k=20,\n        do_sample=False,\n    )\n    '''\n\n    output = model1.generate(\n        **inputs,\n        max_new_tokens=1024,\n        temperature=0.6,\n        top_p=0.95,\n        top_k=20,\n        do_sample=False,  # ho·∫∑c False t√πy b·∫°n\n        output_scores=True,\n        return_dict_in_generate=True,\n    )\n\n    \n    # L·∫•y chu·ªói token sinh ra\n    sequences = output.sequences[0]   # batch=1\n    decoded = tokenizer1.decode(sequences, skip_special_tokens=False)\n    \n    # T√≠nh x√°c su·∫•t\n    all_probs = []\n    for i, score in enumerate(output.scores):\n        # score: [batch, vocab_size]\n        probs = F.softmax(score, dim=-1)  # x√°c su·∫•t\n        token_id = sequences[inputs[\"input_ids\"].shape[1] + i]  # token sinh ·ªü step n√†y\n        token_prob = probs[0, token_id].item()\n        all_probs.append((token_id, token_prob, tokenizer1.decode([token_id])))\n\n\n    #decoded = tokenizer1.decode(output_ids[0], skip_special_tokens=True)\n    return decoded, all_probs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T14:39:34.991165Z","iopub.execute_input":"2025-09-08T14:39:34.991456Z","iopub.status.idle":"2025-09-08T14:39:34.998821Z","shell.execute_reply.started":"2025-09-08T14:39:34.991436Z","shell.execute_reply":"2025-09-08T14:39:34.998162Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"def generate_answer_refine(context, question, options, reasoning):\n    options_text = \"\\n\".join([f\"{i+1}. {opt}\" for i, opt in enumerate(options)])\n\n\n    problem = (\n            f\"{context}\\n\\n\"\n            f\"C√¢u h·ªèi: {question}\\n\"\n            f\"Suy lu·∫≠n chi ti·∫øt cho t·ª´ng l·ª±a ch·ªçn:\\n{reasoning.strip()}\\n\"\n            f\"C√°c l·ª±a ch·ªçn:\\n{options_text}\\n\\n\"\n            \"Nhi·ªám v·ª•:\\n\"\n            \"1. ƒê·ªçc ph·∫ßn suy lu·∫≠n c·ªßa t·ª´ng l·ª±a ch·ªçn.\\n\"\n            \"2. Ch·ªâ g√°n nh√£n 1 cho ph∆∞∆°ng √°n ƒë∆∞·ª£c t·ªïng k·∫øt l√† h·ª£p l√Ω nh·∫•t.\\n\"\n            \"3. T·∫•t c·∫£ c√°c l·ª±a ch·ªçn kh√°c g√°n nh√£n 0, ngay c·∫£ khi suy lu·∫≠n n√≥i 'c√≥ th·ªÉ h·ª£p l√Ω trong ƒëi·ªÅu ki·ªán ƒë·∫∑c bi·ªát'.\\n\"\n            \"4. K·∫øt qu·∫£ ph·∫£i ho√†n to√†n nh·∫•t qu√°n v·ªõi ph·∫ßn t·ªïng k·∫øt suy lu·∫≠n.\\n\"\n            \"5. N·∫øu d·ª±a v√†o context v√† question m√† kh√¥ng th·ªÉ x√°c ƒë·ªãnh option n√†o h·ª£p l√Ω nh·∫•t, g√°n 0 cho t·∫•t c·∫£ c√°c option.\\n\"\n            \"6. Li·ªát k√™ nh√£n t∆∞∆°ng ·ª©ng c·ªßa t·ª´ng l·ª±a ch·ªçn theo ƒë√∫ng th·ª© t·ª±, trong th·∫ª <labels>, v√≠ d·ª•: <labels>1 0 0 0</labels>.\"\n        )\n\n\n    messages = [{\"role\": \"user\", \"content\": problem}]\n    text = tokenizer1.apply_chat_template(\n            messages,\n            tokenize=False,\n            add_generation_prompt=True,\n            enable_thinking=False,\n    )\n    # Tokenize & gen\n    inputs = tokenizer1(text, return_tensors=\"pt\").to(\"cuda\")\n\n    output = model1.generate(\n        **inputs,\n        max_new_tokens=1024,\n        temperature=0.6,\n        top_p=0.95,\n        top_k=20,\n        do_sample=False,  # ho·∫∑c False t√πy b·∫°n\n        output_scores=True,\n        return_dict_in_generate=True,\n    )\n\n    \n    # L·∫•y chu·ªói token sinh ra\n    sequences = output.sequences[0]   # batch=1\n    decoded = tokenizer1.decode(sequences, skip_special_tokens=False)\n    \n    # T√≠nh x√°c su·∫•t\n    all_probs = []\n    for i, score in enumerate(output.scores):\n        # score: [batch, vocab_size]\n        probs = F.softmax(score, dim=-1)  # x√°c su·∫•t\n        token_id = sequences[inputs[\"input_ids\"].shape[1] + i]  # token sinh ·ªü step n√†y\n        token_prob = probs[0, token_id].item()\n        all_probs.append((token_id, token_prob, tokenizer1.decode([token_id])))\n\n\n    #decoded = tokenizer1.decode(output_ids[0], skip_special_tokens=True)\n    return decoded, all_probs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T03:41:49.650699Z","iopub.execute_input":"2025-09-09T03:41:49.651405Z","iopub.status.idle":"2025-09-09T03:41:49.668159Z","shell.execute_reply.started":"2025-09-09T03:41:49.651369Z","shell.execute_reply":"2025-09-09T03:41:49.667337Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def find_option_span(tokens_with_probs, option):\n    \"\"\"\n    Tr·∫£ v·ªÅ (start_idx, end_idx) trong tokens_with_probs n·∫øu t√¨m th·∫•y option,\n    ng∆∞·ª£c l·∫°i tr·∫£ v·ªÅ None.\n    \"\"\"\n    token_texts = [t for _, _, t in tokens_with_probs]\n    option_tokens = list(option)  # naive t√°ch theo k√Ω t·ª±\n    \n    # Gh√©p l·∫°i r·ªìi split ƒë·ªÉ kh·ªõp \"3 gi·ªù\"\n    joined = \"\".join(token_texts)\n    pos = joined.find(option)\n    if pos == -1:\n        return None\n    \n    # Map l·∫°i pos t·ª´ char -> token index\n    char_pos = 0\n    for i, tok in enumerate(token_texts):\n        next_pos = char_pos + len(tok)\n        if char_pos <= pos < next_pos:\n            start_idx = i\n        if char_pos < pos + len(option) <= next_pos:\n            end_idx = i + 1\n            return (start_idx, end_idx)\n        char_pos = next_pos\n    \n    return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T07:36:11.272911Z","iopub.execute_input":"2025-09-09T07:36:11.273208Z","iopub.status.idle":"2025-09-09T07:36:11.285285Z","shell.execute_reply.started":"2025-09-09T07:36:11.273184Z","shell.execute_reply":"2025-09-09T07:36:11.284576Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def find_cutoff_index(tokens_with_probs, cutoff_phrase=\"T·ªïng k·∫øt\"):\n    \"\"\"\n    T√¨m index token n∆°i xu·∫•t hi·ªán cutoff_phrase (\"T·ªïng k·∫øt\").\n    N·∫øu kh√¥ng th·∫•y th√¨ fallback v·ªÅ </think> ho·∫∑c <labels>.\n    \"\"\"\n    token_texts = [t for _, _, t in tokens_with_probs]\n    joined = \"\".join(token_texts)\n\n    # T√¨m theo string\n    pos = joined.find(cutoff_phrase)\n    if pos != -1:\n        # Map t·ª´ char pos -> token index\n        char_pos = 0\n        for i, tok in enumerate(token_texts):\n            next_pos = char_pos + len(tok)\n            if char_pos <= pos < next_pos:\n                return i\n            char_pos = next_pos\n\n    # fallback\n    for i, tok in enumerate(token_texts):\n        if \"</think>\" in tok or \"<labels>\" in tok:\n            return i\n    return len(tokens_with_probs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T07:38:23.823273Z","iopub.execute_input":"2025-09-09T07:38:23.824307Z","iopub.status.idle":"2025-09-09T07:38:23.836627Z","shell.execute_reply.started":"2025-09-09T07:38:23.824275Z","shell.execute_reply":"2025-09-09T07:38:23.835842Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def extract_option_probs(tokens_with_probs, options):\n    texts = [tok for _, _, tok in tokens_with_probs]\n    full_text = \"\".join(texts)\n\n    # t√¨m v·ªã tr√≠ <labels> cu·ªëi c√πng\n    start = full_text.lower().rfind(\"<labels>\")\n    end   = full_text.lower().rfind(\"</labels>\")\n    if start == -1 or end == -1 or end <= start:\n        #print(\"[WARN] Kh√¥ng t√¨m th·∫•y <labels> ho·∫∑c </labels>\")\n        return {}\n\n    # map offset -> token index\n    idx_map = []\n    pos = 0\n    for i, t in enumerate(texts):\n        idx_map.append((pos, pos+len(t), i))\n        pos += len(t)\n\n    start_token = min(i for s, e, i in idx_map if s >= start)\n    end_token   = max(i for s, e, i in idx_map if e <= end)\n    segment = tokens_with_probs[start_token:end_token+1]\n\n    labels = []\n    for _, prob, tok in segment:\n        norm_tok = tok.strip().lower().strip(\",\")\n        if norm_tok in (\"yes\", \"no\"):\n            labels.append((norm_tok, prob))\n\n    #if len(labels) != len(options):\n        #print(f\"[WARN] S·ªë label ({len(labels)}) != s·ªë option ({len(options)})\")\n\n    results = {}\n    for opt, (lab, pr) in zip(options, labels):\n        #print(f\"[DEBUG] Option {opt} -> label={lab}, prob={pr:.4f}\")\n        if lab == \"yes\":\n            results[opt] = (pr, 1-pr)\n        else:\n            results[opt] = (1-pr, pr)\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T14:39:34.999624Z","iopub.execute_input":"2025-09-08T14:39:35.000089Z","iopub.status.idle":"2025-09-08T14:39:37.129395Z","shell.execute_reply.started":"2025-09-08T14:39:35.000064Z","shell.execute_reply":"2025-09-08T14:39:37.128606Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import re\n\ndef extract_think_content(predicted_labels: str) -> str:\n    \"\"\"\n    Tr√≠ch xu·∫•t n·ªôi dung trong th·∫ª <think>...</think>\n    \"\"\"\n    match = re.search(r\"<think>(.*?)</think>\", predicted_labels, re.DOTALL | re.IGNORECASE)\n    if match:\n        return match.group(1).strip()\n    return \"\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T14:39:37.131217Z","iopub.execute_input":"2025-09-08T14:39:37.131555Z","iopub.status.idle":"2025-09-08T14:39:38.223400Z","shell.execute_reply.started":"2025-09-08T14:39:37.131525Z","shell.execute_reply":"2025-09-08T14:39:38.222406Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import math\nimport numpy as np\n\ndef softmax_binary(logits_yes, logits_no):\n    \"\"\"Softmax cho 2 nh√£n yes/no\"\"\"\n    exp_yes = math.exp(logits_yes)\n    exp_no  = math.exp(logits_no)\n    s = exp_yes + exp_no\n    return exp_yes/s, exp_no/s\n\ndef ensemble_sweep_label(probs_cot, probs_refine, options, start=0.05, end=1.0, step=0.05, eps=1e-8):\n    \"\"\"\n    Ensemble CoT + Refine v·ªõi log + weighted sum + softmax nh·ªã ph√¢n\n    Tr·∫£ v·ªÅ list: [(w_refine, final_pred, final_probs, labels)]\n    \"\"\"\n    results = []\n    weights = [round(start + step*i, 2) for i in range(int((end-start)/step)+1)]\n    \n    for w_refine in weights:\n        w_cot = 1.0 - w_refine\n        \n        final_probs = {}\n        labels = []\n        \n        # Duy·ªát t·ª´ng option\n        for opt in options:\n            # Logits\n            log_yes = math.log(probs_cot[opt][0]+eps)*w_cot + math.log(probs_refine[opt][0]+eps)*w_refine\n            log_no  = math.log(probs_cot[opt][1]+eps)*w_cot + math.log(probs_refine[opt][1]+eps)*w_refine\n            \n            # Softmax nh·ªã ph√¢n\n            prob_yes, prob_no = softmax_binary(log_yes, log_no)\n            final_probs[opt] = prob_yes  # ch·ªâ l∆∞u x√°c su·∫•t yes\n            \n            # Nh√£n\n            label = 'yes' if prob_yes > prob_no else 'no'\n            labels.append(label)\n        \n        # final_pred: option c√≥ x√°c su·∫•t yes cao nh·∫•t\n        final_pred = max(final_probs, key=final_probs.get)\n        \n        results.append((w_refine, final_pred, final_probs, labels))\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T14:39:38.224424Z","iopub.execute_input":"2025-09-08T14:39:38.224709Z","iopub.status.idle":"2025-09-08T14:39:39.205074Z","shell.execute_reply.started":"2025-09-08T14:39:38.224683Z","shell.execute_reply":"2025-09-08T14:39:39.204169Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"'''\nquestion = \"Ho√†n th√†nh d·ª± √°n c√° nh√¢n c·∫ßn kho·∫£ng th·ªùi gian bao l√¢u?\"\ncontext = \"\"\"\nT√¥i v·ª´a ho√†n th√†nh d·ª± √°n c√° nh√¢n m√† m√¨nh ƒë√£ b·∫Øt ƒë·∫ßu t·ª´ l√¢u.\n\"\"\"\noptions = [\n\"3 gi·ªù\",\n\"5 ng√†y\",\n\"2 th√°ng\",\n\"6 th√°ng\"\n]\nanswer_cot , prob_cot= generate_answer_cot(context, question, options)\nold_reason_cot = extract_think_content(str(answer_cot)).split(\"<think>\")[-1]\nanswer_refine , prob_refine = generate_answer_refine(context, question, options, old_reason_cot)\n\nprobs_cot = extract_option_probs(prob_cot, options)\nprobs_refine = extract_option_probs(prob_refine, options)\n\nensemble_sweep_label(probs_cot, probs_refine, options)\n'''","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T14:39:39.205917Z","iopub.execute_input":"2025-09-08T14:39:39.206144Z","iopub.status.idle":"2025-09-08T14:39:39.222159Z","shell.execute_reply.started":"2025-09-08T14:39:39.206126Z","shell.execute_reply":"2025-09-08T14:39:39.221459Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"from datasets import load_dataset\n\ndataset = load_dataset(\"thailevann/durationQA_track8_vlsp2025\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-09T07:24:11.802736Z","iopub.execute_input":"2025-09-09T07:24:11.802950Z","iopub.status.idle":"2025-09-09T07:24:17.599573Z","shell.execute_reply.started":"2025-09-09T07:24:11.802933Z","shell.execute_reply":"2025-09-09T07:24:17.598773Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import json\nfrom tqdm import tqdm\nimport os\nimport time\n\noutput_dir = \"results_by_weight\"\nos.makedirs(output_dir, exist_ok=True)\n\nsamples = dataset['test']\n\nprobs_cot_all = {}\nprobs_refine_all = {}\n\ncot_file = os.path.join(output_dir, \"results_cot.jsonl\")\n\ntotal_pred_time = 0.0\n\nwith open(cot_file, \"w\", encoding=\"utf-8\") as f_cot:\n    for sample in tqdm(samples, desc=\"Processing samples\"):\n        qid = sample['qid']\n        context = sample['context']\n        question = sample['question']\n        options = sample['options']\n\n        t0 = time.time()\n        # --- CoT ---\n        answer_cot, prob_cot = generate_answer_cot(context, question, options)\n        probs_cot = extract_option_probs(prob_cot, options)\n        probs_cot_all[qid] = probs_cot\n        labels_cot = ['yes' if probs_cot[opt][0] > 0.5 else 'no' for opt in options]\n        out_cot = {\"qid\": qid, \"labels\": labels_cot, \"context\": context, \"question\": question,\"options\": options}\n        f_cot.write(json.dumps(out_cot, ensure_ascii=False) + \"\\n\")\n\n        # --- Refine ---\n        old_reason_cot = extract_think_content(str(answer_cot)).split(\"<think>\")[-1]\n        answer_refine, prob_refine = generate_answer_refine(context, question, options, old_reason_cot)\n        probs_refine = extract_option_probs(prob_refine, options)\n        probs_refine_all[qid] = probs_refine\n\n        # --- Ensemble sweep v√† ghi file ngay ---\n        results = ensemble_sweep_label(probs_cot, probs_refine, options)\n        for w_refine, pred, final_probs, labels in results:\n            filename = os.path.join(output_dir, f\"results_wrefine_{w_refine:.2f}.jsonl\")\n            with open(filename, \"a\", encoding=\"utf-8\") as f:\n                f.write(json.dumps({\"qid\": qid, \"labels\": labels, \"context\": context, \"question\": question,\"options\": options}, ensure_ascii=False) + \"\\n\")\n\n            # N·∫øu nh√£n kh√°c CoT th√¨ in ra\n            if labels != labels_cot:\n                print(f\"qid={qid} | CoT={labels_cot} -> Ensemble={labels} | w_refine={w_refine}\")\n\n        t1 = time.time()\n        total_pred_time += t1 - t0\n\n# --- Th·ªùi gian trung b√¨nh predict/sample ---\navg_time = total_pred_time / len(samples)\nprint(f\"\\nAverage prediction time per sample: {avg_time:.4f} seconds\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-05T05:16:29.542049Z","iopub.execute_input":"2025-09-05T05:16:29.542798Z","iopub.status.idle":"2025-09-05T05:16:29.549967Z","shell.execute_reply.started":"2025-09-05T05:16:29.542767Z","shell.execute_reply":"2025-09-05T05:16:29.549110Z"}},"outputs":[],"execution_count":17}]}